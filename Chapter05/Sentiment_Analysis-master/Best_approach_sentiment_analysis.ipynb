{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data and binary glove model saved in form of .npy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "Loaded the word vectors!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "wordsList = np.load('./data/wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load('./data/wordVectors.npy')\n",
    "print ('Loaded the word vectors!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(len(wordsList))\n",
    "print(wordVectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.93270004,  1.04209995, -0.78514999,  0.91033   ,  0.22711   ,\n",
       "       -0.62158   , -1.64929998,  0.07686   , -0.58679998,  0.058831  ,\n",
       "        0.35628   ,  0.68915999, -0.50598001,  0.70472997,  1.26639998,\n",
       "       -0.40031001, -0.020687  ,  0.80862999, -0.90565997, -0.074054  ,\n",
       "       -0.87674999, -0.62910002, -0.12684999,  0.11524   , -0.55685002,\n",
       "       -1.68260002, -0.26291001,  0.22632   ,  0.713     , -1.08280003,\n",
       "        2.12310004,  0.49869001,  0.066711  , -0.48225999, -0.17896999,\n",
       "        0.47699001,  0.16384   ,  0.16537   , -0.11506   , -0.15962   ,\n",
       "       -0.94926   , -0.42833   , -0.59456998,  1.35660005, -0.27506   ,\n",
       "        0.19918001, -0.36008   ,  0.55667001, -0.70314997,  0.17157   ], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseballIndex = wordsList.index('baseball')\n",
    "wordVectors[baseballIndex]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[    41    804 201534   1005     15   7446      5  13767      0      0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "maxSeqLength = 10 #Maximum length of sentence\n",
    "numDimensions = 300 #Dimensions for each word vector\n",
    "firstSentence = np.zeros((maxSeqLength), dtype='int32')\n",
    "firstSentence[0] = wordsList.index(\"i\")\n",
    "firstSentence[1] = wordsList.index(\"thought\")\n",
    "firstSentence[2] = wordsList.index(\"the\")\n",
    "firstSentence[3] = wordsList.index(\"movie\")\n",
    "firstSentence[4] = wordsList.index(\"was\")\n",
    "firstSentence[5] = wordsList.index(\"incredible\")\n",
    "firstSentence[6] = wordsList.index(\"and\")\n",
    "firstSentence[7] = wordsList.index(\"inspiring\")\n",
    "#firstSentence[8] and firstSentence[9] are going to be 0\n",
    "print(firstSentence.shape)\n",
    "print(firstSentence) #Shows the row index for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,firstSentence).eval().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n",
      "('The total number of files is', 25000)\n",
      "('The total number of words in the files is', 5844464)\n",
      "('The average number of words in the files is', 233)\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "positiveFiles = ['./data/positiveReviews/' + f for f in listdir('./data/positiveReviews/') if isfile(join('./data/positiveReviews/', f))]\n",
    "negativeFiles = ['./data/negativeReviews/' + f for f in listdir('./data/negativeReviews/') if isfile(join('./data/negativeReviews/', f))]\n",
    "numWords = []\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\") as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\") as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEKCAYAAADenhiQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nObmYmVnlnFzMzKxyTi5mZla5/w+bvdrW6fGbcgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1e109f8fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1200, 0, 8000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxSeqLength = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Secret of Kells is an independent, animated feature that gives us one of the fabled stories surrounding the Book of Kells, an illuminated manuscript from the Middle Ages featuring the four Gospels of the New Testament. I didn't know that this book actually exists, but knowing it now makes my interpretation and analysis much a lot easier. There are a few stories and ideas floating around about how the book came to be, who wrote it, and how it has survived over 1,000 years. This is one of them.<br /><br />We are introduced to Brendan, an orphan who lives at the Abbey of Kells in Ireland with his uncle, Abbot Cellach (voiced by Brendan Gleeson). Abbot Cellach is constructing a massive wall around the abbey to protect the villagers and monks. Brendan is not fond of the wall and neither are the other monks. They are more focused on reading and writing, something Abbot Cellach does not have time for anymore. He fears the \"Northmen,\" those who plunder and leave towns and villages empty and burnt to the ground.<br /><br />One day a traveler comes from the island of Iona near Scotland. It is Brother Aidan, a very wise man who carries with him a special book that is not yet finished. Abbot Cellach grants him permission to stay and Brendan buddies up with him. Aidan has special plans for Brendan. First he needs ink for the book, but he requires specific berries. The only way to get them is to venture outside the walls and into the forest, an area off limits to Brendan. Seeing that he is the only chance for Aidan to continue his work, he decides to sneak out and return with the berries before his uncle notices his absence.<br /><br />In the forest Brendan meets Ashley, the protector of the forest. She allows Brendan passage to the berries and along the way becomes akin to his company. She warns him of the looming danger in the dark and not to foil with it. There are things worse than Vikings out there. From there Brendan is met with more challenges with the book and the looming certainty of invasion.<br /><br />I like the story a lot more now that I know what it is about. Knowing now what the Book of Kells is and what it contains, the animation makes perfect sense. I'm sure you have seen pictures or copies of old texts from hundreds of years ago, with frilly borders, colorful pictures, and extravagant patterns, creatures, and writings adorning the pages. Much like the opening frames of Disney's The Sword in the Stone. The animation here contains a lot of similar designs and patterns. It creates a very unique viewing experience where the story and the animation almost try to outdo each other.<br /><br />I couldn't take my eyes off of the incredible detail. This is some of the finest 2D animation I have seen in years. It's vibrant, stimulating, and full of life. The characters are constantly surrounded by designs, doodles, and patterns in trees, on the walls, and in the air just floating around. It enhances the film.<br /><br />The story is satisfactory, although I think the ending could have been strung out a little more. With a runtime of only 75 minutes I think there could have been something special in the final act. It doesn't give a lot of information nor does it allude to the significance of the book. We are reminded of it's importance but never fully understand. We are told that it gives hope, but never why or how. That was really the only lacking portion of the film. Otherwise I thought the story was interesting though completely outdone by the animation.<br /><br />I guess that's okay to a certain degree. The animation can carry a film so far before it falls short. The story lacks a few parts, but it is an interesting take on a fascinating piece of history. I would recommend looking up briefly the Book of Kells just to get an idea of what myself and this film are talking about. I think it will help your viewing experience a lot more. This a very impressive and beautifully illustrated film that should definitely not be missed.\n"
     ]
    }
   ],
   "source": [
    "fname = positiveFiles[3] #Can use any valid index (not just 3)\n",
    "with open(fname) as f:\n",
    "    for lines in f:\n",
    "        print(lines)\n",
    "        exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in a pre-computed IDs matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "# fileCounter = 0\n",
    "# for pf in positiveFiles:\n",
    "#    with open(pf, \"r\") as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1 \n",
    "\n",
    "# for nf in negativeFiles:\n",
    "#    with open(nf, \"r\") as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1 \n",
    "# #Pass into embedding function and see if it evaluates. \n",
    "# np.save('idsMatrix', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids = np.load('./data/idsMatrix.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Train and Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            num = randint(1,11499)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(13499,24999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(11499,13499)\n",
    "        if (num <= 12499):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start building neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to models/pretrained_lstm.ckpt-10000\n",
      "saved to models/pretrained_lstm.ckpt-20000\n",
      "saved to models/pretrained_lstm.ckpt-30000\n",
      "saved to models/pretrained_lstm.ckpt-40000\n",
      "saved to models/pretrained_lstm.ckpt-50000\n",
      "saved to models/pretrained_lstm.ckpt-60000\n",
      "saved to models/pretrained_lstm.ckpt-70000\n",
      "saved to models/pretrained_lstm.ckpt-80000\n",
      "saved to models/pretrained_lstm.ckpt-90000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "   #Next Batch of reviews\n",
    "   nextBatch, nextBatchLabels = getTrainBatch();\n",
    "   sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "   \n",
    "   #Write summary to Tensorboard\n",
    "   if (i % 50 == 0):\n",
    "       summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "       writer.add_summary(summary, i)\n",
    "\n",
    "   #Save the network every 10,000 training iterations\n",
    "   if (i % 10000 == 0 and i != 0):\n",
    "       save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "       print(\"saved to %s\" % save_path)\n",
    "writer.close()\n",
    "\n",
    "# After running this cell use the following command to see the training progres on tensorboard.\n",
    "# Step 1: go to the directory where you save this ipython notebook and execute this command\n",
    "# Step 2: $tensorboard --logdir=tensorboard \n",
    "# Step 3: there is one link is coming where tensorboard running usually it is http://localhost:6006/\n",
    "# After sept 2 in my case the link is poping with the following message.\n",
    "           # this will Starting TensorBoard 54 at http://jalaj:6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/pretrained_lstm.ckpt-90000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate test accuracy for testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy for this batch:', 87.5)\n",
      "('Accuracy for this batch:', 83.333337306976318)\n",
      "('Accuracy for this batch:', 75.0)\n",
      "('Accuracy for this batch:', 91.666668653488159)\n",
      "('Accuracy for this batch:', 87.5)\n",
      "('Accuracy for this batch:', 83.333337306976318)\n",
      "('Accuracy for this batch:', 91.666668653488159)\n",
      "('Accuracy for this batch:', 83.333337306976318)\n",
      "('Accuracy for this batch:', 79.166668653488159)\n",
      "('Accuracy for this batch:', 91.666668653488159)\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numDimensions = 300\n",
    "maxSeqLength = 250\n",
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "wordsList = np.load('./data/wordsList.npy').tolist()\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load('./data/wordVectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "\n",
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)\n",
    "\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.25)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/pretrained_lstm.ckpt-90000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "def getSentenceMatrix(sentence):\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    sentenceMatrix = np.zeros([batchSize,maxSeqLength], dtype='int32')\n",
    "    cleanedSentence = cleanSentences(sentence)\n",
    "    split = cleanedSentence.split()\n",
    "    for indexCounter,word in enumerate(split):\n",
    "        try:\n",
    "            sentenceMatrix[0,indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            sentenceMatrix[0,indexCounter] = 399999 #Vector for unkown words\n",
    "    return sentenceMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputText = \"That movie was terrible.\"\n",
    "inputMatrix = getSentenceMatrix(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Sentiment\n"
     ]
    }
   ],
   "source": [
    "predictedSentiment = sess.run(prediction, {input_data: inputMatrix})[0]\n",
    "# predictedSentiment[0] represents output score for positive sentiment\n",
    "# predictedSentiment[1] represents output score for negative sentiment\n",
    "\n",
    "if (predictedSentiment[0] > predictedSentiment[1]):\n",
    "    print \"Positive Sentiment\"\n",
    "else:\n",
    "    print \"Negative Sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "secondInputText = \"That movie was the best one I have ever seen.\"\n",
    "secondInputMatrix = getSentenceMatrix(secondInputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Sentiment\n"
     ]
    }
   ],
   "source": [
    "predictedSentiment = sess.run(prediction, {input_data: secondInputMatrix})[0]\n",
    "if (predictedSentiment[0] > predictedSentiment[1]):\n",
    "    print \"Positive Sentiment\"\n",
    "else:\n",
    "    print \"Negative Sentiment\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thirdInputText = \"That movie was not that grate.\"\n",
    "thirdInputMatrix = getSentenceMatrix(thirdInputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Sentiment\n"
     ]
    }
   ],
   "source": [
    "predictedSentiment = sess.run(prediction, {input_data: thirdInputMatrix})[0]\n",
    "if (predictedSentiment[0] > predictedSentiment[1]):\n",
    "    print \"Positive Sentiment\"\n",
    "else:\n",
    "    print \"Negative Sentiment\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
